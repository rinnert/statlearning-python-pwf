{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Introduction to Statistical Learning, Lab 11.1\n",
    "\n",
    "# Classifier Training Example\n",
    "\n",
    "We use the PyTorch library:\n",
    " \n",
    "  - PyTorch [homepage](https://pytorch.org/)\n",
    "  - PyTorch [documentation](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "In this example we train a multi-class classifier.  The reading of input data, torch data set provider and the model definition are provided by imported modules.\n",
    "\n",
    "The raw input data is simply a binary blob of several hundred thousand `np.float32` values in the range \\[-1.0, 1.0\\). Their meaning is defined by a function in  the `dataprovider` module (users can supply their own callables). More specifically, the numbers are interpreted as cartesian (x1, x2) coordinates.  \n",
    "\n",
    "The function `truth_labels_unit_circle` in the `dataprovider` module defines three classes for the (x1, x2) tuples:\n",
    "\n",
    " 1. points outside the unit circle `r = sqrt(x1*x1 + x2*x2) >= 1` (this is considered 'background')\n",
    " 2. points in the unit circle with `x2 <= 0.0` (this is considered 'signal 1')\n",
    " 3. points in the unit circle with `x2 > 0.0` (this is considered 'signal 2')\n",
    " \n",
    "The truth labels are therefore one-hot vectors of dimension 3.  The `dataprovider` dynamically generates the truth labels using the above definition. That is, the truth labels are not stored in the data file, allowing you to use the same data to experiment with different classification problems by simply supplying an appropriate function. In your application the truth lables are likely to be derived from the labeled input data (MC sample). That is not a problem -- simply provide a callable that generates them from your event structure and you are all set.\n",
    "\n",
    "The way the data is read from the file is unecessarily complicated for this simple example. We define a generator function that reads (x1, x2) pairs one-by-one from the file and then re-assemble the points into `np.arrays` and `torch.tensors`.  We could just directly read the whole binary blob into an `np.array` or `torch.tensor` and then reshape it to our needs. However, real life data sets are rarely that easy to access and we would like to provide examples of all necessary components to read more complicated event-oriented data (here an \"event\" is just an (x1, x2) pair).\n",
    "\n",
    "The classifier model is a fully-connected feed-forward neural network. The network layout (number of hidden layers etc. is specified when the model instance is constructed. This makes it straight forward to try different network layouts. The number of input and output nodes must always match the dimension of the parameter space and the number of classes, respectively.\n",
    "In order to predict probabilities, the output layer activation is a *sigmoid* function:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 - e^{-x}}\n",
    "$$\n",
    "\n",
    "We encourage you to experiment with different class definitions, network layouts and other meta parameters. Note that the default classes in this example ensure a reasonably balanced number of class instances.  This is important: if your real life data set is highly imbalanced (e.g. much more background than signal) you *should enforce a balanced data set for training*.\n",
    "\n",
    "Make sure you have a good look at the `dataprovider` and `classification` modules and try to understand what they do.  The `dataprovider` module is not specific to PyTorch in any way -- it is just an example with all the necessary hooks to implement your own data provider suitable for your data source. Read the applicaple PyTorch documentation for the code in the `classification` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Setup\n",
    "Import the required modules and make sure our (and only our!) modules are reloaded before code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%aimport dataprovider, classification\n",
    "%autoreload 1\n",
    "\n",
    "# framework modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import torch\n",
    "import torch.utils.data as utils_data\n",
    "import torch.nn as nn\n",
    "\n",
    "# our modules\n",
    "import dataprovider as dp\n",
    "import classification as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Data Sets\n",
    "Here we generate a binary file with a data set for training and provide PyTorch data sets for training and validation by reading the file. The file reading is handled by a generator function in the `dataprovider` module which provides an iterator over 'events' in the file.\n",
    "\n",
    "When training classifiers it is quite important to have a reasonably balanced data set. That is, all classes should be represented in roughly equal proportions -- if the data set is, say, completelely dominated by background the NN will just learn that there is no signal.\n",
    "\n",
    "In order not to make the example too complicated, we do not take any special measures to ensure this balance as the example data set is roughly balanced by its definition and generation.\n",
    "\n",
    "We split the labeled data sample in a training an validation sample. This can be done in various ways. Here we simply read distinct events from the generated binary file. However you do this, it is important that the training and validation data sets are distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data_file_path = './training_data.bin'\n",
    "dp.generate_data(data_file_path, force=False)\n",
    "num_events = 20000\n",
    "validation_fraction = 0.2\n",
    "num_training_events = int(num_events * (1 - validation_fraction))\n",
    "num_validation_events = num_events - num_training_events\n",
    "training_events = dp.events(data_file_path, evt_max=num_training_events)\n",
    "training_dataset = cl.LabeledDataset(training_events,\n",
    "                                     var_generator=dp.event_to_values,\n",
    "                                     label_generator=dp.truth_labels_unit_circle) \n",
    "validation_events = dp.events(data_file_path, evt_max=num_validation_events, evt_skip=num_training_events)\n",
    "validation_dataset = cl.LabeledDataset(validation_events,\n",
    "                                       var_generator=dp.event_to_values,\n",
    "                                       label_generator=dp.truth_labels_unit_circle) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### A Peep at the Data\n",
    "\n",
    "Let's have a quick look at the data provided by the validation data set (the format is the same for the training data set). We see that the `LabeledDataset` class provides a tuple of of `torch.tensor` representations. The first entry contains the values corresponding to the event and the second contains the associated truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print('Number of events in validation data set: {}'.format(len(validation_dataset)))\n",
    "print(validation_dataset[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### A Graphical Look at the Data\n",
    "Now let's make a plot: we'll make a scatter plot color-coded by the truth labels in the validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "xs = validation_dataset.xs\n",
    "ys = validation_dataset.ys\n",
    "bg_idxs = ys[:,0].nonzero()\n",
    "s1_idxs = ys[:,1].nonzero()\n",
    "s2_idxs = ys[:,2].nonzero()\n",
    "\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.scatter(xs[:,0][bg_idxs], xs[:,1][bg_idxs])\n",
    "plt.scatter(xs[:,0][s1_idxs], xs[:,1][s1_idxs])\n",
    "plt.scatter(xs[:,0][s2_idxs], xs[:,1][s2_idxs])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "#### Training\n",
    "\n",
    "We configure the the network layout and the PyTorch \n",
    "data loader instances.  The network takes two input\n",
    "variables, three outputs corresponding to the three classes and two\n",
    "hidden layers. The network is very small because the classification problem\n",
    "is very simple and we want to see womething not quite perfect in the evaluation\n",
    "plots (separate notebook).  That is, we deliberately limit the network to\n",
    "make the example more interesting.\n",
    "You should experiment with different network layouts and \n",
    "learning rates in the optimizer. Observe how stable you training is by running it several\n",
    "times in the same configuration.\n",
    "\n",
    "The training loop monitors the test loss at the end of each epoch. If there\n",
    "is nothing more to learn (the loss does not significantly decrease over several epochs)\n",
    "the training finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_nn = cl.ClassifierNN(layout=(2, 5, 7, 3))\n",
    "classifier_nn.train()\n",
    "print(classifier_nn)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = utils_data.DataLoader(training_dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True)\n",
    "test_loader = utils_data.DataLoader(validation_dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True)\n",
    "\n",
    "max_epochs = 200\n",
    "min_gain = 0.01\n",
    "grace_limit = 4\n",
    "grace = 0\n",
    "learning_rate = 0.002\n",
    "train_history = []\n",
    "test_history = []\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(classifier_nn.parameters(),\n",
    "                             lr=learning_rate, \n",
    "                             weight_decay=1e-5)\n",
    "\n",
    "best_loss = 1.0\n",
    "for epoch in range(max_epochs):\n",
    "    mean_train_loss = 0.0\n",
    "    for i, (xs, ys) in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # reset gradients\n",
    "        outputs = classifier_nn(xs) \n",
    "        train_loss = loss_function(outputs, ys)\n",
    "        train_loss.backward() # gradient back propagation\n",
    "        optimizer.step()\n",
    "        mean_train_loss = (mean_train_loss * i + train_loss.item()) / (i + 1)\n",
    "    train_history.append(mean_train_loss)\n",
    "    \n",
    "    mean_test_loss = 0.0\n",
    "    for i, (xs, ys) in enumerate(test_loader):\n",
    "        outputs = classifier_nn(xs)\n",
    "        eval_loss = loss_function(outputs, ys)\n",
    "        mean_test_loss = (mean_test_loss * i + eval_loss) / (i + 1)\n",
    "    test_history.append(mean_test_loss)\n",
    "    \n",
    "    print('Epoch {}, mean train/test loss: {:.4f}/{:.4f}'.format(epoch, mean_train_loss, mean_test_loss))\n",
    "    if (best_loss - mean_test_loss) / best_loss < min_gain:\n",
    "        if grace == 0:\n",
    "            print('Entering grace period (limit {})'.format(grace_limit))\n",
    "            grace += 1\n",
    "        elif grace < grace_limit:\n",
    "            grace += 1\n",
    "        else:\n",
    "            print('Nothing more to learn. Training finished.')\n",
    "            break\n",
    "    else:\n",
    "        if grace > 0:\n",
    "            grace = 0\n",
    "            print('Survived grace period.')\n",
    "        best_loss = mean_test_loss\n",
    "else:\n",
    "    print('Maximum number of epochs ({}) reached. Training terminated.'.format(max_epochs))\n",
    "\n",
    "weight_path = classifier_nn.save_weights(tag='example1', time_stamp=False)\n",
    "print(\"Saved network parameters to '{}'.\".format(weight_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_history, label='Training Loss')\n",
    "plt.plot(test_history, label='Test Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflor version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfmodel = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation=\"relu\", input_shape=[2]),\n",
    "    tf.keras.layers.Dense(7, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfloss = \"binary_crossentropy\"\n",
    "tfopt = tf.keras.optimizers.Adam(learning_rate, decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfmodel.compile(optimizer=tfopt,\n",
    "              loss=tfloss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloss = []\n",
    "evalloss  = []\n",
    "best_loss = 1\n",
    "for epoch in range(1, max_epochs):\n",
    "    tffit = tfmodel.fit(training_dataset.xs, training_dataset.ys, epochs=1, shuffle=True, batch_size=batch_size)\n",
    "    trainloss.append(tffit.history[\"loss\"][0])\n",
    "    evalloss.append(tfmodel.evaluate(validation_dataset.xs, validation_dataset.ys))\n",
    "    \n",
    "    mean_train_loss = np.mean(trainloss)\n",
    "    mean_test_loss  = np.mean(evalloss)\n",
    "    print('Epoch {}, mean train/test loss: {:.4f}/{:.4f}'.format(epoch, mean_train_loss, mean_test_loss))\n",
    "    if (best_loss - mean_test_loss) / best_loss < min_gain:\n",
    "        if grace == 0:\n",
    "            print('Entering grace period (limit {})'.format(grace_limit))\n",
    "            grace += 1\n",
    "        elif grace < grace_limit:\n",
    "            grace += 1\n",
    "        else:\n",
    "            print('Nothing more to learn. Training finished.')\n",
    "            break\n",
    "    else:\n",
    "        if grace > 0:\n",
    "            grace = 0\n",
    "            print('Survived grace period.')\n",
    "        best_loss = mean_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainloss)\n",
    "plt.plot(evalloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfmodel.save(\"./classifier_weights_example1_2x5x7x3.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
